import os

#---------------------------------------------------------------------
# THINGS THAT CHANGE A LOT
#---------------------------------------------------------------------
EXP_NAME = "char1"
NUM_EPOCHS = 40 # train for 40 epochs
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# Common Constants
#---------------------------------------------------------------------
max_vocab_size = {"en" : 200000, "fr" : 200000}

# special vocabulary constants
PAD = b"_PAD"
GO = b"_GO"
EOS = b"_EOS"
UNK = b"_UNK"
START_VOCAB = [PAD, GO, EOS, UNK]
PAD_ID = 0
GO_ID = 1
EOS_ID = 2
UNK_ID = 3

# attention constants
NO_ATTN = 0
SOFT_ATTN = 1
attn_post = ["NO_ATTN", "SOFT_ATTN"]
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# Model Parameters
#---------------------------------------------------------------------
num_layers_enc = 3
num_layers_dec = 3
use_attn = SOFT_ATTN
hidden_units = 400
load_existing_model = True
create_buckets_flag = True
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# Data Parameters
#---------------------------------------------------------------------
NUM_SENTENCES = 50000
model_dir = "in_en_model_char_{0:d}".format(NUM_SENTENCES)
input_dir = "in_en_data_char_{0:d}".format(NUM_SENTENCES)
data_dir  = "in_en_data_char"

NUM_TRAINING_SENTENCES = (NUM_SENTENCES * 90) // 100
NUM_DEV_SENTENCES = 500
FREQ_THRESH = 0
BATCH_SIZE = 64
BUCKET_WIDTH = 10
NUM_BUCKETS = 20
MAX_PREDICT_LEN = BUCKET_WIDTH*NUM_BUCKETS

if not os.path.exists(model_dir):
    os.makedirs(model_dir)
if not os.path.exists(input_dir):
    print("Input folder not found".format(input_dir))

text_fname = {"en": os.path.join(input_dir, "text.en"), "fr": os.path.join(input_dir, "text.fr")}
bucket_data_fname = os.path.join(input_dir, "buckets_{0:d}.list")
tokens_fname = os.path.join(input_dir, "tokens.list")
vocab_path = os.path.join(input_dir, "vocab.dict")
w2i_path = os.path.join(input_dir, "w2i.dict")
i2w_path = os.path.join(input_dir, "i2w.dict")
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# Training EPOCHS
#---------------------------------------------------------------------
# if 0 - will only load a previously saved model if it exists
#---------------------------------------------------------------------
# Change the dev set to include all the sentences not used for training,
# instead of 500. Using all during training impacts timing
if NUM_EPOCHS == 0:
    NUM_DEV_SENTENCES = NUM_SENTENCES-NUM_TRAINING_SENTENCES
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# GPU/CPU
#---------------------------------------------------------------------
# if >= 0, use GPU, if negative use CPU
gpuid = -1
#---------------------------------------------------------------------


#---------------------------------------------------------------------
# Log file details
#---------------------------------------------------------------------
name_to_log = "{0:d}sen_{1:d}-{2:d}layers_{3:d}units_{4:s}_{5:s}".format(
    NUM_SENTENCES, num_layers_enc, num_layers_dec, hidden_units, EXP_NAME, attn_post[use_attn])
log_train_fil_name = os.path.join(model_dir, "train_{0:s}.log".format(name_to_log))
log_dev_fil_name = os.path.join(model_dir, "dev_{0:s}.log".format(name_to_log))
model_fil = os.path.join(model_dir, "seq2seq_{0:s}.model".format(name_to_log))
#---------------------------------------------------------------------
