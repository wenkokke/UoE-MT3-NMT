Inuktitut English dataset configuration
vocab size, en=9363, fr=64858
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.model
here False
Splitting data into 10 buckets, each of width=3
Saving bucket data
Bucket 3, # items=17421
Bucket 6, # items=12852
Bucket 9, # items=3520
Bucket 12, # items=2669
Bucket 15, # items=2405
Bucket 18, # items=2119
Bucket 21, # items=2052
Bucket 24, # items=1687
Bucket 27, # items=1510
Bucket 30, # items=3765
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 118.802195
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_1.model
BLEU: 9.24
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 85.428666
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_2.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 65.888304
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_3.model
BLEU: 10.86
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 61.525738
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_4.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 53.513445
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_5.model
BLEU: 12.00
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 49.203059
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_6.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 56.561989
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_7.model
BLEU: 12.96
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 57.361945
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_8.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 54.531016
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_9.model
BLEU: 15.22
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 62.677550
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_10.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 67.157549
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_11.model
BLEU: 14.41
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 58.157351
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_12.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 62.997875
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_13.model
BLEU: 15.11
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 59.153796
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_14.model
finished training on 45000 sentences
--------------------------------------------------
computing perplexity
--------------------------------------------------
dev perplexity | 70.308979
# words in dev |   3331
--------------------------------------------------
Saving model
Finished saving model
wooohooo!
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN_15.model
BLEU: 16.25
Simple predictions (╯°□°）╯︵ ┻━┻
training set predictions
English predictions, s=0, num=2:
--------------------------------------------------
sentence: 0
Src | hansard                                                                         
Ref | hansard                                                                         
Hyp | public of contents _EOS                                                         
--------------------------------------------------
precision | 0.0000
recall | 0.0000
--------------------------------------------------
sentence: 1
Src | nunavut kanata                                                                  
Ref | nunavut canada                                                                  
Hyp | nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut nunavut _EOS
--------------------------------------------------
precision | 0.0588
recall | 0.5000
sentences matching filter = 2
Simple predictions (╯°□°）╯︵ ┻━┻
dev set predictions
English predictions, s=45000, num=3:
--------------------------------------------------
sentence: 45000
Src | isumajunga minisitaujuq tunisijunnarpa uvattinnut ilanginnik uattiarurnisavinirnik ammalu qanuilijjutigijanginnik tamatumunga ilaguttiutijausimajumut akilirsuinnariaqarutinginnut tamanna pitaqariaqalaurmat kiinaujanik ammalu aulattiutinginnik titirarvingmi
Ref | i wonder if the minister can give us some background and rationale for this additional expenditure that is required for finance and administration
Hyp | they mr minister that that that that some some some some some some some some some and funding the the
--------------------------------------------------
precision | 0.2500
recall | 0.2174
--------------------------------------------------
sentence: 45001
Src | qujannamiingujutit                                                              
Ref | thank you                                                                       
Hyp | thank you thank you thank you thank you thank thank thank thank thank thank thank thank thank thank _EOS
--------------------------------------------------
precision | 0.1111
recall | 1.0000
--------------------------------------------------
sentence: 45002
Src | iksivautaq tusaajikkut                                                          
Ref | chairperson interpretation                                                      
Hyp | chairperson interpretation chairperson interpretation chairperson interpretation mr mr mr interpretation interpretation interpretation interpretation interpretation interpretation interpretation interpretation interpretation interpretation _EOS
--------------------------------------------------
precision | 0.1053
recall | 1.0000
sentences matching filter = 3
--------------------------------------------------
BLEU: 16.25
--------------------------------------------------
Final saving model
Finished saving model
in_en_model_50000/train_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/dev_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.log
in_en_model_50000/seq2seq_50000sen_3-3layers_200units_base1_aajuq_SOFT_ATTN.model
BLEU: 16.25
